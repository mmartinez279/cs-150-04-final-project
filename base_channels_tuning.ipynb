{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4604f89",
   "metadata": {},
   "source": [
    "# Base Channels Tuning\n",
    "\n",
    "This file contains the results of tuning the `base_channels` hyperparameter in the UNet architecture. The `base_channels` parameter controls the number of channels in the first layer of the encoder, and all subsequent layers scale proportionally. We are testing the following values: 8, 16, 32, 64, 96, and 128. The results are found below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7020af",
   "metadata": {},
   "source": [
    "## Base Channels = 8\n",
    "\n",
    "This cell implements the UNet architecture with **`base_channels = 8`**. The `base_channels` parameter determines the number of feature channels in the first convolutional layer of the encoder, and all subsequent layers scale proportionally.\n",
    "\n",
    "**Architecture Details:**\n",
    "\n",
    "With `base_channels = 8`, the channel progression through the UNet encoder is:\n",
    "- Initial layer: $8$ channels\n",
    "- Encoder layer 1: $8 \\rightarrow 16$ channels\n",
    "- Encoder layer 2: $16 \\rightarrow 32$ channels  \n",
    "- Encoder layer 3: $32 \\rightarrow 64$ channels\n",
    "- Bottleneck: $64$ channels\n",
    "- Decoder layers: $64 \\rightarrow 32 \\rightarrow 16 \\rightarrow 8$ channels\n",
    "\n",
    "**Model Characteristics:**\n",
    "- **Total Parameters:** Smallest model size (~few hundred thousand parameters)\n",
    "- **Memory Usage:** Lowest memory footprint\n",
    "- **Training Speed:** Fastest training and inference\n",
    "- **Representation Capacity:** Limited - may struggle with complex patterns\n",
    "\n",
    "This is the most lightweight configuration, suitable for quick experimentation or resource-constrained environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UT Zappos50K Shoe Diffusion Model - base_channels = 8\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Config & Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Dataset Loading (EXACT REPLICATION)\n",
    "# -----------------------------\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"aryashah2k/large-shoe-dataset-ut-zappos50k\")\n",
    "\n",
    "BASE_DIR = \"/root/.cache/kagglehub/datasets/aryashah2k/large-shoe-dataset-ut-zappos50k/versions/1\"\n",
    "IMAGE_ROOT = os.path.join(BASE_DIR, \"ut-zap50k-images-square\")  # we'll use the square images\n",
    "\n",
    "if not os.path.exists(IMAGE_ROOT):\n",
    "    raise FileNotFoundError(f\"IMAGE_ROOT '{IMAGE_ROOT}' does not exist. Check your base path.\")\n",
    "\n",
    "print(\"Image root:\", IMAGE_ROOT)\n",
    "print(\"Contents:\", os.listdir(IMAGE_ROOT)[:10])\n",
    "\n",
    "# Collect all image paths recursively from IMAGE_ROOT\n",
    "extensions = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\", \"*.tiff\", \"*.webp\")\n",
    "image_paths = []\n",
    "for ext in extensions:\n",
    "    image_paths.extend(glob.glob(os.path.join(IMAGE_ROOT, \"**\", ext), recursive=True))\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(f\"No image files found under {IMAGE_ROOT} with extensions {extensions}.\")\n",
    "\n",
    "print(f\"Found {len(image_paths)} image files.\")\n",
    "\n",
    "# Transform: resize & normalize (NO cropping, just warp to 64x64)\n",
    "image_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),                     # [0,1]\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),    # [-1,1]\n",
    "])\n",
    "\n",
    "class ZapposImageDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        from PIL import Image\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        # unconditional -> we don't care about labels, return dummy\n",
    "        return img, 0\n",
    "\n",
    "dataset = ZapposImageDataset(image_paths, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Dataset and DataLoader ready.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Time Embedding (Sinusoidal)\n",
    "# -----------------------------\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: (B,) int or float timesteps\n",
    "        returns: (B, dim)\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb_factor = math.log(10000) / (half_dim - 1)\n",
    "        exponents = torch.exp(torch.arange(half_dim, device=device) * -emb_factor)\n",
    "        t = t.float().unsqueeze(1)  # (B,1)\n",
    "        angles = t * exponents[None, :]  # (B, half_dim)\n",
    "        emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1))\n",
    "        return emb\n",
    "\n",
    "# -----------------------------\n",
    "# 3. UNet Building Blocks\n",
    "# -----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        self.time_mlp = nn.Linear(time_dim, out_channels)\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.res_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.res_conv = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        t_emb: (B, time_dim)\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        t_added = self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = h + t_added\n",
    "        h = self.act(self.norm1(h))\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.act(self.norm2(h))\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(in_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "        self.down = nn.Conv2d(out_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        skip = x\n",
    "        x = self.down(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_channels, time_dim):\n",
    "        \"\"\"\n",
    "        in_channels: from below\n",
    "        out_channels: after upsample\n",
    "        skip_channels: from skip connection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "        self.res1 = ResidualBlock(out_channels + skip_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "\n",
    "    def forward(self, x, skip, t_emb):\n",
    "        x = self.up(x)\n",
    "        # fix size mismatch if any (shouldn't happen with 64x64 -> 8x8 pyramid, but safe)\n",
    "        if x.size(-1) != skip.size(-1):\n",
    "            diff = skip.size(-1) - x.size(-1)\n",
    "            x = F.pad(x, (diff // 2, diff - diff // 2, diff // 2, diff - diff // 2))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 4. UNet for 64x64 RGB Shoes (Unconditional)\n",
    "# -----------------------------\n",
    "class ShoeUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=64, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        # Time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_dim),\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim),\n",
    "        )\n",
    "\n",
    "        # Initial conv\n",
    "        self.init_conv = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "\n",
    "        # Encoder: 64x64 -> 32x32 -> 16x16 -> 8x8\n",
    "        chs = [\n",
    "            base_channels,\n",
    "            base_channels * 2,\n",
    "            base_channels * 4,\n",
    "            base_channels * 8,\n",
    "        ]  # [8,16,32,64]\n",
    "\n",
    "        self.down1 = DownBlock(chs[0], chs[1], time_dim)  # 8->16\n",
    "        self.down2 = DownBlock(chs[1], chs[2], time_dim)  # 16->32\n",
    "        self.down3 = DownBlock(chs[2], chs[3], time_dim)  # 32->64\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(chs[3], chs[3], time_dim)\n",
    "\n",
    "        # Decoder: 8x8 -> 16x16 -> 32x32 -> 64x64\n",
    "        self.up3 = UpBlock(chs[3], chs[2], skip_channels=chs[3], time_dim=time_dim)\n",
    "        self.up2 = UpBlock(chs[2], chs[1], skip_channels=chs[2], time_dim=time_dim)\n",
    "        self.up1 = UpBlock(chs[1], chs[0], skip_channels=chs[1], time_dim=time_dim)\n",
    "\n",
    "        # Final conv: predict noise ε\n",
    "        self.out_conv = nn.Conv2d(base_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W)\n",
    "        t: (B,) timesteps\n",
    "        \"\"\"\n",
    "        t_emb = self.time_mlp(t)  # (B, time_dim)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        x, skip1 = self.down1(x, t_emb)  # 64x64 -> 32x32\n",
    "        x, skip2 = self.down2(x, t_emb)  # 32x32 -> 16x16\n",
    "        x, skip3 = self.down3(x, t_emb)  # 16x16 -> 8x8\n",
    "\n",
    "        x = self.bottleneck(x, t_emb)\n",
    "\n",
    "        x = self.up3(x, skip3, t_emb)    # 8x8 -> 16x16\n",
    "        x = self.up2(x, skip2, t_emb)    # 16x16 -> 32x32\n",
    "        x = self.up1(x, skip1, t_emb)    # 32x32 -> 64x64\n",
    "\n",
    "        x = self.out_conv(x)             # predict noise\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 5. DDPM Diffusion Utilities\n",
    "# -----------------------------\n",
    "class Diffusion:\n",
    "    def __init__(self, num_steps=1000, beta_start=1e-4, beta_end=0.02, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_steps, device=device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alpha_cumprod_prev = torch.cat(\n",
    "            [torch.tensor([1.0], device=device), self.alpha_cumprod[:-1]], dim=0\n",
    "        )\n",
    "\n",
    "        self.sqrt_alphas = torch.sqrt(self.alphas)\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "\n",
    "        self.posterior_variance = (\n",
    "            self.betas\n",
    "            * (1.0 - self.alpha_cumprod_prev)\n",
    "            / (1.0 - self.alpha_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.clamp(self.posterior_variance, min=1e-20)\n",
    "        )\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_steps, (batch_size,), device=self.device).long()\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: q(x_t | x_0)\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_bar_t = self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_bar_t * x0 + sqrt_one_minus_alpha_bar_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t):\n",
    "        \"\"\"\n",
    "        Reverse step: p(x_{t-1} | x_t)\n",
    "        \"\"\"\n",
    "        B = x_t.size(0)\n",
    "        t_batch = torch.full((B,), t, device=self.device, dtype=torch.long)\n",
    "\n",
    "        eps_theta = model(x_t, t_batch)\n",
    "\n",
    "        beta_t = self.betas[t]\n",
    "        alpha_t = self.alphas[t]\n",
    "        alpha_cumprod_t = self.alpha_cumprod[t]\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alpha_cumprod[t]\n",
    "        sqrt_recip_alpha_t = 1.0 / torch.sqrt(alpha_t)\n",
    "\n",
    "        model_mean = sqrt_recip_alpha_t * (\n",
    "            x_t - (beta_t / sqrt_one_minus_alpha_cumprod_t) * eps_theta\n",
    "        )\n",
    "\n",
    "        if t == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_var_t = self.posterior_variance[t]\n",
    "            noise = torch.randn_like(x_t)\n",
    "            return model_mean + torch.sqrt(posterior_var_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=8):\n",
    "        \"\"\"\n",
    "        Sample x_0 from the model by starting from pure noise.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        x = torch.randn(batch_size, 3, image_size, image_size, device=self.device)\n",
    "\n",
    "        for t in reversed(range(self.num_steps)):\n",
    "            x = self.p_sample(model, x, t)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Instantiate Model & Diffusion\n",
    "# -----------------------------\n",
    "time_dim = 256\n",
    "model = ShoeUNet(\n",
    "    in_channels=3,\n",
    "    base_channels=8,\n",
    "    time_dim=time_dim\n",
    ").to(device)\n",
    "\n",
    "diffusion = Diffusion(\n",
    "    num_steps=1000,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Training Loop\n",
    "# -----------------------------\n",
    "num_epochs = 30  # increase for better visuals\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, _ in train_loader:   # ignore labels\n",
    "        x = x.to(device)        # already normalized to [-1,1]\n",
    "\n",
    "        b = x.size(0)\n",
    "        t = diffusion.sample_timesteps(b)\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        x_t = diffusion.q_sample(x, t, noise=noise)\n",
    "\n",
    "        pred_noise = model(x_t, t)\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * b\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Sampling & Visualization\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def show_samples(model, diffusion, n=8):\n",
    "    \"\"\"\n",
    "    Generate and show shoe samples from the diffusion model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples = diffusion.sample(model, image_size=image_size, batch_size=n)\n",
    "    samples = (samples.clamp(-1, 1) + 1) / 2.0  # back to [0,1]\n",
    "\n",
    "    grid = utils.make_grid(samples, nrow=min(n, 4))\n",
    "    plt.figure(figsize=(4 * (n // 4 + 1), 4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nBase Channels = 8 Results:\")\n",
    "show_samples(model, diffusion, n=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50470b13",
   "metadata": {},
   "source": [
    "## Base Channels = 16\n",
    "\n",
    "This cell implements the UNet architecture with **`base_channels = 16`**.\n",
    "\n",
    "**Architecture Details:**\n",
    "\n",
    "With `base_channels = 16`, the channel progression through the UNet encoder is:\n",
    "- Initial layer: $16$ channels\n",
    "- Encoder layer 1: $16 \\rightarrow 32$ channels\n",
    "- Encoder layer 2: $32 \\rightarrow 64$ channels  \n",
    "- Encoder layer 3: $64 \\rightarrow 128$ channels\n",
    "- Bottleneck: $128$ channels\n",
    "- Decoder layers: $128 \\rightarrow 64 \\rightarrow 32 \\rightarrow 16$ channels\n",
    "\n",
    "**Model Characteristics:**\n",
    "- **Total Parameters:** Small model size (~1-2 million parameters)\n",
    "- **Memory Usage:** Low memory footprint\n",
    "- **Training Speed:** Fast training and inference\n",
    "- **Representation Capacity:** Moderate - good balance for many tasks\n",
    "\n",
    "This configuration provides a good starting point for experimentation with reasonable capacity while maintaining efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UT Zappos50K Shoe Diffusion Model - base_channels = 16\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Config & Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Dataset Loading (EXACT REPLICATION)\n",
    "# -----------------------------\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"aryashah2k/large-shoe-dataset-ut-zappos50k\")\n",
    "\n",
    "BASE_DIR = \"/root/.cache/kagglehub/datasets/aryashah2k/large-shoe-dataset-ut-zappos50k/versions/1\"\n",
    "IMAGE_ROOT = os.path.join(BASE_DIR, \"ut-zap50k-images-square\")  # we'll use the square images\n",
    "\n",
    "if not os.path.exists(IMAGE_ROOT):\n",
    "    raise FileNotFoundError(f\"IMAGE_ROOT '{IMAGE_ROOT}' does not exist. Check your base path.\")\n",
    "\n",
    "print(\"Image root:\", IMAGE_ROOT)\n",
    "print(\"Contents:\", os.listdir(IMAGE_ROOT)[:10])\n",
    "\n",
    "# Collect all image paths recursively from IMAGE_ROOT\n",
    "extensions = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\", \"*.tiff\", \"*.webp\")\n",
    "image_paths = []\n",
    "for ext in extensions:\n",
    "    image_paths.extend(glob.glob(os.path.join(IMAGE_ROOT, \"**\", ext), recursive=True))\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(f\"No image files found under {IMAGE_ROOT} with extensions {extensions}.\")\n",
    "\n",
    "print(f\"Found {len(image_paths)} image files.\")\n",
    "\n",
    "# Transform: resize & normalize (NO cropping, just warp to 64x64)\n",
    "image_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),                     # [0,1]\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),    # [-1,1]\n",
    "])\n",
    "\n",
    "class ZapposImageDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        from PIL import Image\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        # unconditional -> we don't care about labels, return dummy\n",
    "        return img, 0\n",
    "\n",
    "dataset = ZapposImageDataset(image_paths, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Dataset and DataLoader ready.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Time Embedding (Sinusoidal)\n",
    "# -----------------------------\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: (B,) int or float timesteps\n",
    "        returns: (B, dim)\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb_factor = math.log(10000) / (half_dim - 1)\n",
    "        exponents = torch.exp(torch.arange(half_dim, device=device) * -emb_factor)\n",
    "        t = t.float().unsqueeze(1)  # (B,1)\n",
    "        angles = t * exponents[None, :]  # (B, half_dim)\n",
    "        emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1))\n",
    "        return emb\n",
    "\n",
    "# -----------------------------\n",
    "# 3. UNet Building Blocks\n",
    "# -----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        self.time_mlp = nn.Linear(time_dim, out_channels)\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.res_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.res_conv = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        t_emb: (B, time_dim)\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        t_added = self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = h + t_added\n",
    "        h = self.act(self.norm1(h))\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.act(self.norm2(h))\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(in_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "        self.down = nn.Conv2d(out_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        skip = x\n",
    "        x = self.down(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_channels, time_dim):\n",
    "        \"\"\"\n",
    "        in_channels: from below\n",
    "        out_channels: after upsample\n",
    "        skip_channels: from skip connection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "        self.res1 = ResidualBlock(out_channels + skip_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "\n",
    "    def forward(self, x, skip, t_emb):\n",
    "        x = self.up(x)\n",
    "        # fix size mismatch if any (shouldn't happen with 64x64 -> 8x8 pyramid, but safe)\n",
    "        if x.size(-1) != skip.size(-1):\n",
    "            diff = skip.size(-1) - x.size(-1)\n",
    "            x = F.pad(x, (diff // 2, diff - diff // 2, diff // 2, diff - diff // 2))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 4. UNet for 64x64 RGB Shoes (Unconditional)\n",
    "# -----------------------------\n",
    "class ShoeUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=64, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        # Time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_dim),\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim),\n",
    "        )\n",
    "\n",
    "        # Initial conv\n",
    "        self.init_conv = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "\n",
    "        # Encoder: 64x64 -> 32x32 -> 16x16 -> 8x8\n",
    "        chs = [\n",
    "            base_channels,\n",
    "            base_channels * 2,\n",
    "            base_channels * 4,\n",
    "            base_channels * 8,\n",
    "        ]  # [16,32,64,128]\n",
    "\n",
    "        self.down1 = DownBlock(chs[0], chs[1], time_dim)  # 16->32\n",
    "        self.down2 = DownBlock(chs[1], chs[2], time_dim)  # 32->64\n",
    "        self.down3 = DownBlock(chs[2], chs[3], time_dim)  # 64->128\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(chs[3], chs[3], time_dim)\n",
    "\n",
    "        # Decoder: 8x8 -> 16x16 -> 32x32 -> 64x64\n",
    "        self.up3 = UpBlock(chs[3], chs[2], skip_channels=chs[3], time_dim=time_dim)\n",
    "        self.up2 = UpBlock(chs[2], chs[1], skip_channels=chs[2], time_dim=time_dim)\n",
    "        self.up1 = UpBlock(chs[1], chs[0], skip_channels=chs[1], time_dim=time_dim)\n",
    "\n",
    "        # Final conv: predict noise ε\n",
    "        self.out_conv = nn.Conv2d(base_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W)\n",
    "        t: (B,) timesteps\n",
    "        \"\"\"\n",
    "        t_emb = self.time_mlp(t)  # (B, time_dim)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        x, skip1 = self.down1(x, t_emb)  # 64x64 -> 32x32\n",
    "        x, skip2 = self.down2(x, t_emb)  # 32x32 -> 16x16\n",
    "        x, skip3 = self.down3(x, t_emb)  # 16x16 -> 8x8\n",
    "\n",
    "        x = self.bottleneck(x, t_emb)\n",
    "\n",
    "        x = self.up3(x, skip3, t_emb)    # 8x8 -> 16x16\n",
    "        x = self.up2(x, skip2, t_emb)    # 16x16 -> 32x32\n",
    "        x = self.up1(x, skip1, t_emb)    # 32x32 -> 64x64\n",
    "\n",
    "        x = self.out_conv(x)             # predict noise\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 5. DDPM Diffusion Utilities\n",
    "# -----------------------------\n",
    "class Diffusion:\n",
    "    def __init__(self, num_steps=1000, beta_start=1e-4, beta_end=0.02, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_steps, device=device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alpha_cumprod_prev = torch.cat(\n",
    "            [torch.tensor([1.0], device=device), self.alpha_cumprod[:-1]], dim=0\n",
    "        )\n",
    "\n",
    "        self.sqrt_alphas = torch.sqrt(self.alphas)\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "\n",
    "        self.posterior_variance = (\n",
    "            self.betas\n",
    "            * (1.0 - self.alpha_cumprod_prev)\n",
    "            / (1.0 - self.alpha_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.clamp(self.posterior_variance, min=1e-20)\n",
    "        )\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_steps, (batch_size,), device=self.device).long()\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: q(x_t | x_0)\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_bar_t = self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_bar_t * x0 + sqrt_one_minus_alpha_bar_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t):\n",
    "        \"\"\"\n",
    "        Reverse step: p(x_{t-1} | x_t)\n",
    "        \"\"\"\n",
    "        B = x_t.size(0)\n",
    "        t_batch = torch.full((B,), t, device=self.device, dtype=torch.long)\n",
    "\n",
    "        eps_theta = model(x_t, t_batch)\n",
    "\n",
    "        beta_t = self.betas[t]\n",
    "        alpha_t = self.alphas[t]\n",
    "        alpha_cumprod_t = self.alpha_cumprod[t]\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alpha_cumprod[t]\n",
    "        sqrt_recip_alpha_t = 1.0 / torch.sqrt(alpha_t)\n",
    "\n",
    "        model_mean = sqrt_recip_alpha_t * (\n",
    "            x_t - (beta_t / sqrt_one_minus_alpha_cumprod_t) * eps_theta\n",
    "        )\n",
    "\n",
    "        if t == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_var_t = self.posterior_variance[t]\n",
    "            noise = torch.randn_like(x_t)\n",
    "            return model_mean + torch.sqrt(posterior_var_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=8):\n",
    "        \"\"\"\n",
    "        Sample x_0 from the model by starting from pure noise.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        x = torch.randn(batch_size, 3, image_size, image_size, device=self.device)\n",
    "\n",
    "        for t in reversed(range(self.num_steps)):\n",
    "            x = self.p_sample(model, x, t)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Instantiate Model & Diffusion\n",
    "# -----------------------------\n",
    "time_dim = 256\n",
    "model = ShoeUNet(\n",
    "    in_channels=3,\n",
    "    base_channels=16,\n",
    "    time_dim=time_dim\n",
    ").to(device)\n",
    "\n",
    "diffusion = Diffusion(\n",
    "    num_steps=1000,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Training Loop\n",
    "# -----------------------------\n",
    "num_epochs = 30  # increase for better visuals\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, _ in train_loader:   # ignore labels\n",
    "        x = x.to(device)        # already normalized to [-1,1]\n",
    "\n",
    "        b = x.size(0)\n",
    "        t = diffusion.sample_timesteps(b)\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        x_t = diffusion.q_sample(x, t, noise=noise)\n",
    "\n",
    "        pred_noise = model(x_t, t)\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * b\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Sampling & Visualization\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def show_samples(model, diffusion, n=8):\n",
    "    \"\"\"\n",
    "    Generate and show shoe samples from the diffusion model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples = diffusion.sample(model, image_size=image_size, batch_size=n)\n",
    "    samples = (samples.clamp(-1, 1) + 1) / 2.0  # back to [0,1]\n",
    "\n",
    "    grid = utils.make_grid(samples, nrow=min(n, 4))\n",
    "    plt.figure(figsize=(4 * (n // 4 + 1), 4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nBase Channels = 16 Results:\")\n",
    "show_samples(model, diffusion, n=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb197a",
   "metadata": {},
   "source": [
    "## Base Channels = 32\n",
    "\n",
    "This cell implements the UNet architecture with **`base_channels = 32`**.\n",
    "\n",
    "**Architecture Details:**\n",
    "\n",
    "With `base_channels = 32`, the channel progression through the UNet encoder is:\n",
    "- Initial layer: $32$ channels\n",
    "- Encoder layer 1: $32 \\rightarrow 64$ channels\n",
    "- Encoder layer 2: $64 \\rightarrow 128$ channels  \n",
    "- Encoder layer 3: $128 \\rightarrow 256$ channels\n",
    "- Bottleneck: $256$ channels\n",
    "- Decoder layers: $256 \\rightarrow 128 \\rightarrow 64 \\rightarrow 32$ channels\n",
    "\n",
    "**Model Characteristics:**\n",
    "- **Total Parameters:** Medium model size (~3-5 million parameters)\n",
    "- **Memory Usage:** Moderate memory footprint\n",
    "- **Training Speed:** Moderate training and inference speed\n",
    "- **Representation Capacity:** Good - suitable for most image generation tasks\n",
    "\n",
    "This configuration offers a solid balance between model capacity and computational efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f7a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UT Zappos50K Shoe Diffusion Model - base_channels = 32\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Config & Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Dataset Loading (EXACT REPLICATION)\n",
    "# -----------------------------\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"aryashah2k/large-shoe-dataset-ut-zappos50k\")\n",
    "\n",
    "BASE_DIR = \"/root/.cache/kagglehub/datasets/aryashah2k/large-shoe-dataset-ut-zappos50k/versions/1\"\n",
    "IMAGE_ROOT = os.path.join(BASE_DIR, \"ut-zap50k-images-square\")  # we'll use the square images\n",
    "\n",
    "if not os.path.exists(IMAGE_ROOT):\n",
    "    raise FileNotFoundError(f\"IMAGE_ROOT '{IMAGE_ROOT}' does not exist. Check your base path.\")\n",
    "\n",
    "print(\"Image root:\", IMAGE_ROOT)\n",
    "print(\"Contents:\", os.listdir(IMAGE_ROOT)[:10])\n",
    "\n",
    "# Collect all image paths recursively from IMAGE_ROOT\n",
    "extensions = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\", \"*.tiff\", \"*.webp\")\n",
    "image_paths = []\n",
    "for ext in extensions:\n",
    "    image_paths.extend(glob.glob(os.path.join(IMAGE_ROOT, \"**\", ext), recursive=True))\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(f\"No image files found under {IMAGE_ROOT} with extensions {extensions}.\")\n",
    "\n",
    "print(f\"Found {len(image_paths)} image files.\")\n",
    "\n",
    "# Transform: resize & normalize (NO cropping, just warp to 64x64)\n",
    "image_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),                     # [0,1]\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),    # [-1,1]\n",
    "])\n",
    "\n",
    "class ZapposImageDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        from PIL import Image\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        # unconditional -> we don't care about labels, return dummy\n",
    "        return img, 0\n",
    "\n",
    "dataset = ZapposImageDataset(image_paths, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Dataset and DataLoader ready.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Time Embedding (Sinusoidal)\n",
    "# -----------------------------\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: (B,) int or float timesteps\n",
    "        returns: (B, dim)\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb_factor = math.log(10000) / (half_dim - 1)\n",
    "        exponents = torch.exp(torch.arange(half_dim, device=device) * -emb_factor)\n",
    "        t = t.float().unsqueeze(1)  # (B,1)\n",
    "        angles = t * exponents[None, :]  # (B, half_dim)\n",
    "        emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1))\n",
    "        return emb\n",
    "\n",
    "# -----------------------------\n",
    "# 3. UNet Building Blocks\n",
    "# -----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        self.time_mlp = nn.Linear(time_dim, out_channels)\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.res_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.res_conv = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        t_emb: (B, time_dim)\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        t_added = self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = h + t_added\n",
    "        h = self.act(self.norm1(h))\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.act(self.norm2(h))\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(in_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "        self.down = nn.Conv2d(out_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        skip = x\n",
    "        x = self.down(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_channels, time_dim):\n",
    "        \"\"\"\n",
    "        in_channels: from below\n",
    "        out_channels: after upsample\n",
    "        skip_channels: from skip connection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "        self.res1 = ResidualBlock(out_channels + skip_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "\n",
    "    def forward(self, x, skip, t_emb):\n",
    "        x = self.up(x)\n",
    "        # fix size mismatch if any (shouldn't happen with 64x64 -> 8x8 pyramid, but safe)\n",
    "        if x.size(-1) != skip.size(-1):\n",
    "            diff = skip.size(-1) - x.size(-1)\n",
    "            x = F.pad(x, (diff // 2, diff - diff // 2, diff // 2, diff - diff // 2))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 4. UNet for 64x64 RGB Shoes (Unconditional)\n",
    "# -----------------------------\n",
    "class ShoeUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=64, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        # Time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_dim),\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim),\n",
    "        )\n",
    "\n",
    "        # Initial conv\n",
    "        self.init_conv = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "\n",
    "        # Encoder: 64x64 -> 32x32 -> 16x16 -> 8x8\n",
    "        chs = [\n",
    "            base_channels,\n",
    "            base_channels * 2,\n",
    "            base_channels * 4,\n",
    "            base_channels * 8,\n",
    "        ]  # [32,64,128,256]\n",
    "\n",
    "        self.down1 = DownBlock(chs[0], chs[1], time_dim)  # 32->64\n",
    "        self.down2 = DownBlock(chs[1], chs[2], time_dim)  # 64->128\n",
    "        self.down3 = DownBlock(chs[2], chs[3], time_dim)  # 128->256\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(chs[3], chs[3], time_dim)\n",
    "\n",
    "        # Decoder: 8x8 -> 16x16 -> 32x32 -> 64x64\n",
    "        self.up3 = UpBlock(chs[3], chs[2], skip_channels=chs[3], time_dim=time_dim)\n",
    "        self.up2 = UpBlock(chs[2], chs[1], skip_channels=chs[2], time_dim=time_dim)\n",
    "        self.up1 = UpBlock(chs[1], chs[0], skip_channels=chs[1], time_dim=time_dim)\n",
    "\n",
    "        # Final conv: predict noise ε\n",
    "        self.out_conv = nn.Conv2d(base_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W)\n",
    "        t: (B,) timesteps\n",
    "        \"\"\"\n",
    "        t_emb = self.time_mlp(t)  # (B, time_dim)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        x, skip1 = self.down1(x, t_emb)  # 64x64 -> 32x32\n",
    "        x, skip2 = self.down2(x, t_emb)  # 32x32 -> 16x16\n",
    "        x, skip3 = self.down3(x, t_emb)  # 16x16 -> 8x8\n",
    "\n",
    "        x = self.bottleneck(x, t_emb)\n",
    "\n",
    "        x = self.up3(x, skip3, t_emb)    # 8x8 -> 16x16\n",
    "        x = self.up2(x, skip2, t_emb)    # 16x16 -> 32x32\n",
    "        x = self.up1(x, skip1, t_emb)    # 32x32 -> 64x64\n",
    "\n",
    "        x = self.out_conv(x)             # predict noise\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 5. DDPM Diffusion Utilities\n",
    "# -----------------------------\n",
    "class Diffusion:\n",
    "    def __init__(self, num_steps=1000, beta_start=1e-4, beta_end=0.02, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_steps, device=device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alpha_cumprod_prev = torch.cat(\n",
    "            [torch.tensor([1.0], device=device), self.alpha_cumprod[:-1]], dim=0\n",
    "        )\n",
    "\n",
    "        self.sqrt_alphas = torch.sqrt(self.alphas)\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "\n",
    "        self.posterior_variance = (\n",
    "            self.betas\n",
    "            * (1.0 - self.alpha_cumprod_prev)\n",
    "            / (1.0 - self.alpha_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.clamp(self.posterior_variance, min=1e-20)\n",
    "        )\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_steps, (batch_size,), device=self.device).long()\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: q(x_t | x_0)\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_bar_t = self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_bar_t * x0 + sqrt_one_minus_alpha_bar_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t):\n",
    "        \"\"\"\n",
    "        Reverse step: p(x_{t-1} | x_t)\n",
    "        \"\"\"\n",
    "        B = x_t.size(0)\n",
    "        t_batch = torch.full((B,), t, device=self.device, dtype=torch.long)\n",
    "\n",
    "        eps_theta = model(x_t, t_batch)\n",
    "\n",
    "        beta_t = self.betas[t]\n",
    "        alpha_t = self.alphas[t]\n",
    "        alpha_cumprod_t = self.alpha_cumprod[t]\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alpha_cumprod[t]\n",
    "        sqrt_recip_alpha_t = 1.0 / torch.sqrt(alpha_t)\n",
    "\n",
    "        model_mean = sqrt_recip_alpha_t * (\n",
    "            x_t - (beta_t / sqrt_one_minus_alpha_cumprod_t) * eps_theta\n",
    "        )\n",
    "\n",
    "        if t == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_var_t = self.posterior_variance[t]\n",
    "            noise = torch.randn_like(x_t)\n",
    "            return model_mean + torch.sqrt(posterior_var_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=8):\n",
    "        \"\"\"\n",
    "        Sample x_0 from the model by starting from pure noise.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        x = torch.randn(batch_size, 3, image_size, image_size, device=self.device)\n",
    "\n",
    "        for t in reversed(range(self.num_steps)):\n",
    "            x = self.p_sample(model, x, t)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Instantiate Model & Diffusion\n",
    "# -----------------------------\n",
    "time_dim = 256\n",
    "model = ShoeUNet(\n",
    "    in_channels=3,\n",
    "    base_channels=32,\n",
    "    time_dim=time_dim\n",
    ").to(device)\n",
    "\n",
    "diffusion = Diffusion(\n",
    "    num_steps=1000,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Training Loop\n",
    "# -----------------------------\n",
    "num_epochs = 30  # increase for better visuals\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, _ in train_loader:   # ignore labels\n",
    "        x = x.to(device)        # already normalized to [-1,1]\n",
    "\n",
    "        b = x.size(0)\n",
    "        t = diffusion.sample_timesteps(b)\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        x_t = diffusion.q_sample(x, t, noise=noise)\n",
    "\n",
    "        pred_noise = model(x_t, t)\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * b\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Sampling & Visualization\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def show_samples(model, diffusion, n=8):\n",
    "    \"\"\"\n",
    "    Generate and show shoe samples from the diffusion model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples = diffusion.sample(model, image_size=image_size, batch_size=n)\n",
    "    samples = (samples.clamp(-1, 1) + 1) / 2.0  # back to [0,1]\n",
    "\n",
    "    grid = utils.make_grid(samples, nrow=min(n, 4))\n",
    "    plt.figure(figsize=(4 * (n // 4 + 1), 4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nBase Channels = 32 Results:\")\n",
    "show_samples(model, diffusion, n=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8212002e",
   "metadata": {},
   "source": [
    "## Base Channels = 64\n",
    "\n",
    "This cell implements the UNet architecture with **`base_channels = 64`** (the baseline configuration from the original model).\n",
    "\n",
    "**Architecture Details:**\n",
    "\n",
    "With `base_channels = 64`, the channel progression through the UNet encoder is:\n",
    "- Initial layer: $64$ channels\n",
    "- Encoder layer 1: $64 \\rightarrow 128$ channels\n",
    "- Encoder layer 2: $128 \\rightarrow 256$ channels  \n",
    "- Encoder layer 3: $256 \\rightarrow 512$ channels\n",
    "- Bottleneck: $512$ channels\n",
    "- Decoder layers: $512 \\rightarrow 256 \\rightarrow 128 \\rightarrow 64$ channels\n",
    "\n",
    "**Model Characteristics:**\n",
    "- **Total Parameters:** Large model size (~10-15 million parameters)\n",
    "- **Memory Usage:** High memory footprint\n",
    "- **Training Speed:** Slower training and inference\n",
    "- **Representation Capacity:** Excellent - strong capacity for complex patterns\n",
    "\n",
    "This is the baseline configuration used in the original model, providing a good balance between quality and computational cost for most applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f436bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UT Zappos50K Shoe Diffusion Model - base_channels = 64\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Config & Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Dataset Loading (EXACT REPLICATION)\n",
    "# -----------------------------\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"aryashah2k/large-shoe-dataset-ut-zappos50k\")\n",
    "\n",
    "BASE_DIR = \"/root/.cache/kagglehub/datasets/aryashah2k/large-shoe-dataset-ut-zappos50k/versions/1\"\n",
    "IMAGE_ROOT = os.path.join(BASE_DIR, \"ut-zap50k-images-square\")  # we'll use the square images\n",
    "\n",
    "if not os.path.exists(IMAGE_ROOT):\n",
    "    raise FileNotFoundError(f\"IMAGE_ROOT '{IMAGE_ROOT}' does not exist. Check your base path.\")\n",
    "\n",
    "print(\"Image root:\", IMAGE_ROOT)\n",
    "print(\"Contents:\", os.listdir(IMAGE_ROOT)[:10])\n",
    "\n",
    "# Collect all image paths recursively from IMAGE_ROOT\n",
    "extensions = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\", \"*.tiff\", \"*.webp\")\n",
    "image_paths = []\n",
    "for ext in extensions:\n",
    "    image_paths.extend(glob.glob(os.path.join(IMAGE_ROOT, \"**\", ext), recursive=True))\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(f\"No image files found under {IMAGE_ROOT} with extensions {extensions}.\")\n",
    "\n",
    "print(f\"Found {len(image_paths)} image files.\")\n",
    "\n",
    "# Transform: resize & normalize (NO cropping, just warp to 64x64)\n",
    "image_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),                     # [0,1]\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),    # [-1,1]\n",
    "])\n",
    "\n",
    "class ZapposImageDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        from PIL import Image\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        # unconditional -> we don't care about labels, return dummy\n",
    "        return img, 0\n",
    "\n",
    "dataset = ZapposImageDataset(image_paths, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Dataset and DataLoader ready.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Time Embedding (Sinusoidal)\n",
    "# -----------------------------\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: (B,) int or float timesteps\n",
    "        returns: (B, dim)\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb_factor = math.log(10000) / (half_dim - 1)\n",
    "        exponents = torch.exp(torch.arange(half_dim, device=device) * -emb_factor)\n",
    "        t = t.float().unsqueeze(1)  # (B,1)\n",
    "        angles = t * exponents[None, :]  # (B, half_dim)\n",
    "        emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1))\n",
    "        return emb\n",
    "\n",
    "# -----------------------------\n",
    "# 3. UNet Building Blocks\n",
    "# -----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        self.time_mlp = nn.Linear(time_dim, out_channels)\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.res_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.res_conv = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        t_emb: (B, time_dim)\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        t_added = self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = h + t_added\n",
    "        h = self.act(self.norm1(h))\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.act(self.norm2(h))\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(in_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "        self.down = nn.Conv2d(out_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        skip = x\n",
    "        x = self.down(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_channels, time_dim):\n",
    "        \"\"\"\n",
    "        in_channels: from below\n",
    "        out_channels: after upsample\n",
    "        skip_channels: from skip connection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "        self.res1 = ResidualBlock(out_channels + skip_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "\n",
    "    def forward(self, x, skip, t_emb):\n",
    "        x = self.up(x)\n",
    "        # fix size mismatch if any (shouldn't happen with 64x64 -> 8x8 pyramid, but safe)\n",
    "        if x.size(-1) != skip.size(-1):\n",
    "            diff = skip.size(-1) - x.size(-1)\n",
    "            x = F.pad(x, (diff // 2, diff - diff // 2, diff // 2, diff - diff // 2))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 4. UNet for 64x64 RGB Shoes (Unconditional)\n",
    "# -----------------------------\n",
    "class ShoeUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=64, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        # Time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_dim),\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim),\n",
    "        )\n",
    "\n",
    "        # Initial conv\n",
    "        self.init_conv = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "\n",
    "        # Encoder: 64x64 -> 32x32 -> 16x16 -> 8x8\n",
    "        chs = [\n",
    "            base_channels,\n",
    "            base_channels * 2,\n",
    "            base_channels * 4,\n",
    "            base_channels * 8,\n",
    "        ]  # [64,128,256,512]\n",
    "\n",
    "        self.down1 = DownBlock(chs[0], chs[1], time_dim)  # 64->128\n",
    "        self.down2 = DownBlock(chs[1], chs[2], time_dim)  # 128->256\n",
    "        self.down3 = DownBlock(chs[2], chs[3], time_dim)  # 256->512\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(chs[3], chs[3], time_dim)\n",
    "\n",
    "        # Decoder: 8x8 -> 16x16 -> 32x32 -> 64x64\n",
    "        self.up3 = UpBlock(chs[3], chs[2], skip_channels=chs[3], time_dim=time_dim)\n",
    "        self.up2 = UpBlock(chs[2], chs[1], skip_channels=chs[2], time_dim=time_dim)\n",
    "        self.up1 = UpBlock(chs[1], chs[0], skip_channels=chs[1], time_dim=time_dim)\n",
    "\n",
    "        # Final conv: predict noise ε\n",
    "        self.out_conv = nn.Conv2d(base_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W)\n",
    "        t: (B,) timesteps\n",
    "        \"\"\"\n",
    "        t_emb = self.time_mlp(t)  # (B, time_dim)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        x, skip1 = self.down1(x, t_emb)  # 64x64 -> 32x32\n",
    "        x, skip2 = self.down2(x, t_emb)  # 32x32 -> 16x16\n",
    "        x, skip3 = self.down3(x, t_emb)  # 16x16 -> 8x8\n",
    "\n",
    "        x = self.bottleneck(x, t_emb)\n",
    "\n",
    "        x = self.up3(x, skip3, t_emb)    # 8x8 -> 16x16\n",
    "        x = self.up2(x, skip2, t_emb)    # 16x16 -> 32x32\n",
    "        x = self.up1(x, skip1, t_emb)    # 32x32 -> 64x64\n",
    "\n",
    "        x = self.out_conv(x)             # predict noise\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 5. DDPM Diffusion Utilities\n",
    "# -----------------------------\n",
    "class Diffusion:\n",
    "    def __init__(self, num_steps=1000, beta_start=1e-4, beta_end=0.02, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_steps, device=device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alpha_cumprod_prev = torch.cat(\n",
    "            [torch.tensor([1.0], device=device), self.alpha_cumprod[:-1]], dim=0\n",
    "        )\n",
    "\n",
    "        self.sqrt_alphas = torch.sqrt(self.alphas)\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "\n",
    "        self.posterior_variance = (\n",
    "            self.betas\n",
    "            * (1.0 - self.alpha_cumprod_prev)\n",
    "            / (1.0 - self.alpha_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.clamp(self.posterior_variance, min=1e-20)\n",
    "        )\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_steps, (batch_size,), device=self.device).long()\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: q(x_t | x_0)\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_bar_t = self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_bar_t * x0 + sqrt_one_minus_alpha_bar_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t):\n",
    "        \"\"\"\n",
    "        Reverse step: p(x_{t-1} | x_t)\n",
    "        \"\"\"\n",
    "        B = x_t.size(0)\n",
    "        t_batch = torch.full((B,), t, device=self.device, dtype=torch.long)\n",
    "\n",
    "        eps_theta = model(x_t, t_batch)\n",
    "\n",
    "        beta_t = self.betas[t]\n",
    "        alpha_t = self.alphas[t]\n",
    "        alpha_cumprod_t = self.alpha_cumprod[t]\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alpha_cumprod[t]\n",
    "        sqrt_recip_alpha_t = 1.0 / torch.sqrt(alpha_t)\n",
    "\n",
    "        model_mean = sqrt_recip_alpha_t * (\n",
    "            x_t - (beta_t / sqrt_one_minus_alpha_cumprod_t) * eps_theta\n",
    "        )\n",
    "\n",
    "        if t == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_var_t = self.posterior_variance[t]\n",
    "            noise = torch.randn_like(x_t)\n",
    "            return model_mean + torch.sqrt(posterior_var_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=8):\n",
    "        \"\"\"\n",
    "        Sample x_0 from the model by starting from pure noise.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        x = torch.randn(batch_size, 3, image_size, image_size, device=self.device)\n",
    "\n",
    "        for t in reversed(range(self.num_steps)):\n",
    "            x = self.p_sample(model, x, t)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Instantiate Model & Diffusion\n",
    "# -----------------------------\n",
    "time_dim = 256\n",
    "model = ShoeUNet(\n",
    "    in_channels=3,\n",
    "    base_channels=64,\n",
    "    time_dim=time_dim\n",
    ").to(device)\n",
    "\n",
    "diffusion = Diffusion(\n",
    "    num_steps=1000,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Training Loop\n",
    "# -----------------------------\n",
    "num_epochs = 30  # increase for better visuals\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, _ in train_loader:   # ignore labels\n",
    "        x = x.to(device)        # already normalized to [-1,1]\n",
    "\n",
    "        b = x.size(0)\n",
    "        t = diffusion.sample_timesteps(b)\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        x_t = diffusion.q_sample(x, t, noise=noise)\n",
    "\n",
    "        pred_noise = model(x_t, t)\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * b\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Sampling & Visualization\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def show_samples(model, diffusion, n=8):\n",
    "    \"\"\"\n",
    "    Generate and show shoe samples from the diffusion model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples = diffusion.sample(model, image_size=image_size, batch_size=n)\n",
    "    samples = (samples.clamp(-1, 1) + 1) / 2.0  # back to [0,1]\n",
    "\n",
    "    grid = utils.make_grid(samples, nrow=min(n, 4))\n",
    "    plt.figure(figsize=(4 * (n // 4 + 1), 4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nBase Channels = 64 Results:\")\n",
    "show_samples(model, diffusion, n=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8744c",
   "metadata": {},
   "source": [
    "## Base Channels = 96\n",
    "\n",
    "This cell implements the UNet architecture with **`base_channels = 96`**.\n",
    "\n",
    "**Architecture Details:**\n",
    "\n",
    "With `base_channels = 96`, the channel progression through the UNet encoder is:\n",
    "- Initial layer: $96$ channels\n",
    "- Encoder layer 1: $96 \\rightarrow 192$ channels\n",
    "- Encoder layer 2: $192 \\rightarrow 384$ channels  \n",
    "- Encoder layer 3: $384 \\rightarrow 768$ channels\n",
    "- Bottleneck: $768$ channels\n",
    "- Decoder layers: $768 \\rightarrow 384 \\rightarrow 192 \\rightarrow 96$ channels\n",
    "\n",
    "**Model Characteristics:**\n",
    "- **Total Parameters:** Very large model size (~20-30 million parameters)\n",
    "- **Memory Usage:** Very high memory footprint\n",
    "- **Training Speed:** Slow training and inference\n",
    "- **Representation Capacity:** Excellent - high capacity for capturing fine details\n",
    "\n",
    "This configuration provides increased model capacity beyond the baseline, potentially improving generation quality at the cost of computational resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebebed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UT Zappos50K Shoe Diffusion Model - base_channels = 96\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Config & Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Dataset Loading (EXACT REPLICATION)\n",
    "# -----------------------------\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"aryashah2k/large-shoe-dataset-ut-zappos50k\")\n",
    "\n",
    "BASE_DIR = \"/root/.cache/kagglehub/datasets/aryashah2k/large-shoe-dataset-ut-zappos50k/versions/1\"\n",
    "IMAGE_ROOT = os.path.join(BASE_DIR, \"ut-zap50k-images-square\")  # we'll use the square images\n",
    "\n",
    "if not os.path.exists(IMAGE_ROOT):\n",
    "    raise FileNotFoundError(f\"IMAGE_ROOT '{IMAGE_ROOT}' does not exist. Check your base path.\")\n",
    "\n",
    "print(\"Image root:\", IMAGE_ROOT)\n",
    "print(\"Contents:\", os.listdir(IMAGE_ROOT)[:10])\n",
    "\n",
    "# Collect all image paths recursively from IMAGE_ROOT\n",
    "extensions = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\", \"*.tiff\", \"*.webp\")\n",
    "image_paths = []\n",
    "for ext in extensions:\n",
    "    image_paths.extend(glob.glob(os.path.join(IMAGE_ROOT, \"**\", ext), recursive=True))\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(f\"No image files found under {IMAGE_ROOT} with extensions {extensions}.\")\n",
    "\n",
    "print(f\"Found {len(image_paths)} image files.\")\n",
    "\n",
    "# Transform: resize & normalize (NO cropping, just warp to 64x64)\n",
    "image_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),                     # [0,1]\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),    # [-1,1]\n",
    "])\n",
    "\n",
    "class ZapposImageDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        from PIL import Image\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        # unconditional -> we don't care about labels, return dummy\n",
    "        return img, 0\n",
    "\n",
    "dataset = ZapposImageDataset(image_paths, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Dataset and DataLoader ready.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Time Embedding (Sinusoidal)\n",
    "# -----------------------------\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: (B,) int or float timesteps\n",
    "        returns: (B, dim)\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb_factor = math.log(10000) / (half_dim - 1)\n",
    "        exponents = torch.exp(torch.arange(half_dim, device=device) * -emb_factor)\n",
    "        t = t.float().unsqueeze(1)  # (B,1)\n",
    "        angles = t * exponents[None, :]  # (B, half_dim)\n",
    "        emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1))\n",
    "        return emb\n",
    "\n",
    "# -----------------------------\n",
    "# 3. UNet Building Blocks\n",
    "# -----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        self.time_mlp = nn.Linear(time_dim, out_channels)\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.res_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.res_conv = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        t_emb: (B, time_dim)\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        t_added = self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = h + t_added\n",
    "        h = self.act(self.norm1(h))\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.act(self.norm2(h))\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(in_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "        self.down = nn.Conv2d(out_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        skip = x\n",
    "        x = self.down(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_channels, time_dim):\n",
    "        \"\"\"\n",
    "        in_channels: from below\n",
    "        out_channels: after upsample\n",
    "        skip_channels: from skip connection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "        self.res1 = ResidualBlock(out_channels + skip_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "\n",
    "    def forward(self, x, skip, t_emb):\n",
    "        x = self.up(x)\n",
    "        # fix size mismatch if any (shouldn't happen with 64x64 -> 8x8 pyramid, but safe)\n",
    "        if x.size(-1) != skip.size(-1):\n",
    "            diff = skip.size(-1) - x.size(-1)\n",
    "            x = F.pad(x, (diff // 2, diff - diff // 2, diff // 2, diff - diff // 2))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 4. UNet for 64x64 RGB Shoes (Unconditional)\n",
    "# -----------------------------\n",
    "class ShoeUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=64, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        # Time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_dim),\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim),\n",
    "        )\n",
    "\n",
    "        # Initial conv\n",
    "        self.init_conv = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "\n",
    "        # Encoder: 64x64 -> 32x32 -> 16x16 -> 8x8\n",
    "        chs = [\n",
    "            base_channels,\n",
    "            base_channels * 2,\n",
    "            base_channels * 4,\n",
    "            base_channels * 8,\n",
    "        ]  # [96,192,384,768]\n",
    "\n",
    "        self.down1 = DownBlock(chs[0], chs[1], time_dim)  # 96->192\n",
    "        self.down2 = DownBlock(chs[1], chs[2], time_dim)  # 192->384\n",
    "        self.down3 = DownBlock(chs[2], chs[3], time_dim)  # 384->768\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(chs[3], chs[3], time_dim)\n",
    "\n",
    "        # Decoder: 8x8 -> 16x16 -> 32x32 -> 64x64\n",
    "        self.up3 = UpBlock(chs[3], chs[2], skip_channels=chs[3], time_dim=time_dim)\n",
    "        self.up2 = UpBlock(chs[2], chs[1], skip_channels=chs[2], time_dim=time_dim)\n",
    "        self.up1 = UpBlock(chs[1], chs[0], skip_channels=chs[1], time_dim=time_dim)\n",
    "\n",
    "        # Final conv: predict noise ε\n",
    "        self.out_conv = nn.Conv2d(base_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W)\n",
    "        t: (B,) timesteps\n",
    "        \"\"\"\n",
    "        t_emb = self.time_mlp(t)  # (B, time_dim)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        x, skip1 = self.down1(x, t_emb)  # 64x64 -> 32x32\n",
    "        x, skip2 = self.down2(x, t_emb)  # 32x32 -> 16x16\n",
    "        x, skip3 = self.down3(x, t_emb)  # 16x16 -> 8x8\n",
    "\n",
    "        x = self.bottleneck(x, t_emb)\n",
    "\n",
    "        x = self.up3(x, skip3, t_emb)    # 8x8 -> 16x16\n",
    "        x = self.up2(x, skip2, t_emb)    # 16x16 -> 32x32\n",
    "        x = self.up1(x, skip1, t_emb)    # 32x32 -> 64x64\n",
    "\n",
    "        x = self.out_conv(x)             # predict noise\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 5. DDPM Diffusion Utilities\n",
    "# -----------------------------\n",
    "class Diffusion:\n",
    "    def __init__(self, num_steps=1000, beta_start=1e-4, beta_end=0.02, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_steps, device=device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alpha_cumprod_prev = torch.cat(\n",
    "            [torch.tensor([1.0], device=device), self.alpha_cumprod[:-1]], dim=0\n",
    "        )\n",
    "\n",
    "        self.sqrt_alphas = torch.sqrt(self.alphas)\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "\n",
    "        self.posterior_variance = (\n",
    "            self.betas\n",
    "            * (1.0 - self.alpha_cumprod_prev)\n",
    "            / (1.0 - self.alpha_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.clamp(self.posterior_variance, min=1e-20)\n",
    "        )\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_steps, (batch_size,), device=self.device).long()\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: q(x_t | x_0)\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_bar_t = self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_bar_t * x0 + sqrt_one_minus_alpha_bar_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t):\n",
    "        \"\"\"\n",
    "        Reverse step: p(x_{t-1} | x_t)\n",
    "        \"\"\"\n",
    "        B = x_t.size(0)\n",
    "        t_batch = torch.full((B,), t, device=self.device, dtype=torch.long)\n",
    "\n",
    "        eps_theta = model(x_t, t_batch)\n",
    "\n",
    "        beta_t = self.betas[t]\n",
    "        alpha_t = self.alphas[t]\n",
    "        alpha_cumprod_t = self.alpha_cumprod[t]\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alpha_cumprod[t]\n",
    "        sqrt_recip_alpha_t = 1.0 / torch.sqrt(alpha_t)\n",
    "\n",
    "        model_mean = sqrt_recip_alpha_t * (\n",
    "            x_t - (beta_t / sqrt_one_minus_alpha_cumprod_t) * eps_theta\n",
    "        )\n",
    "\n",
    "        if t == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_var_t = self.posterior_variance[t]\n",
    "            noise = torch.randn_like(x_t)\n",
    "            return model_mean + torch.sqrt(posterior_var_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=8):\n",
    "        \"\"\"\n",
    "        Sample x_0 from the model by starting from pure noise.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        x = torch.randn(batch_size, 3, image_size, image_size, device=self.device)\n",
    "\n",
    "        for t in reversed(range(self.num_steps)):\n",
    "            x = self.p_sample(model, x, t)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Instantiate Model & Diffusion\n",
    "# -----------------------------\n",
    "time_dim = 256\n",
    "model = ShoeUNet(\n",
    "    in_channels=3,\n",
    "    base_channels=96,\n",
    "    time_dim=time_dim\n",
    ").to(device)\n",
    "\n",
    "diffusion = Diffusion(\n",
    "    num_steps=1000,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Training Loop\n",
    "# -----------------------------\n",
    "num_epochs = 30  # increase for better visuals\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, _ in train_loader:   # ignore labels\n",
    "        x = x.to(device)        # already normalized to [-1,1]\n",
    "\n",
    "        b = x.size(0)\n",
    "        t = diffusion.sample_timesteps(b)\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        x_t = diffusion.q_sample(x, t, noise=noise)\n",
    "\n",
    "        pred_noise = model(x_t, t)\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * b\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Sampling & Visualization\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def show_samples(model, diffusion, n=8):\n",
    "    \"\"\"\n",
    "    Generate and show shoe samples from the diffusion model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples = diffusion.sample(model, image_size=image_size, batch_size=n)\n",
    "    samples = (samples.clamp(-1, 1) + 1) / 2.0  # back to [0,1]\n",
    "\n",
    "    grid = utils.make_grid(samples, nrow=min(n, 4))\n",
    "    plt.figure(figsize=(4 * (n // 4 + 1), 4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nBase Channels = 96 Results:\")\n",
    "show_samples(model, diffusion, n=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd3814",
   "metadata": {},
   "source": [
    "## Base Channels = 128\n",
    "\n",
    "This cell implements the UNet architecture with **`base_channels = 128`**.\n",
    "\n",
    "**Architecture Details:**\n",
    "\n",
    "With `base_channels = 128`, the channel progression through the UNet encoder is:\n",
    "- Initial layer: $128$ channels\n",
    "- Encoder layer 1: $128 \\rightarrow 256$ channels\n",
    "- Encoder layer 2: $256 \\rightarrow 512$ channels  \n",
    "- Encoder layer 3: $512 \\rightarrow 1024$ channels\n",
    "- Bottleneck: $1024$ channels\n",
    "- Decoder layers: $1024 \\rightarrow 512 \\rightarrow 256 \\rightarrow 128$ channels\n",
    "\n",
    "**Model Characteristics:**\n",
    "- **Total Parameters:** Largest model size (~40-60 million parameters)\n",
    "- **Memory Usage:** Very high memory footprint (may require GPU with significant VRAM)\n",
    "- **Training Speed:** Slowest training and inference\n",
    "- **Representation Capacity:** Maximum - highest capacity for capturing complex patterns and fine details\n",
    "\n",
    "This is the largest configuration tested, providing maximum model capacity at the cost of significant computational resources. The bottleneck layer reaches 1024 channels, which is a common maximum in many high-capacity vision models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f6c20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UT Zappos50K Shoe Diffusion Model - base_channels = 128\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Config & Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Dataset Loading (EXACT REPLICATION)\n",
    "# -----------------------------\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"aryashah2k/large-shoe-dataset-ut-zappos50k\")\n",
    "\n",
    "BASE_DIR = \"/root/.cache/kagglehub/datasets/aryashah2k/large-shoe-dataset-ut-zappos50k/versions/1\"\n",
    "IMAGE_ROOT = os.path.join(BASE_DIR, \"ut-zap50k-images-square\")  # we'll use the square images\n",
    "\n",
    "if not os.path.exists(IMAGE_ROOT):\n",
    "    raise FileNotFoundError(f\"IMAGE_ROOT '{IMAGE_ROOT}' does not exist. Check your base path.\")\n",
    "\n",
    "print(\"Image root:\", IMAGE_ROOT)\n",
    "print(\"Contents:\", os.listdir(IMAGE_ROOT)[:10])\n",
    "\n",
    "# Collect all image paths recursively from IMAGE_ROOT\n",
    "extensions = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\", \"*.tiff\", \"*.webp\")\n",
    "image_paths = []\n",
    "for ext in extensions:\n",
    "    image_paths.extend(glob.glob(os.path.join(IMAGE_ROOT, \"**\", ext), recursive=True))\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(f\"No image files found under {IMAGE_ROOT} with extensions {extensions}.\")\n",
    "\n",
    "print(f\"Found {len(image_paths)} image files.\")\n",
    "\n",
    "# Transform: resize & normalize (NO cropping, just warp to 64x64)\n",
    "image_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),                     # [0,1]\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),    # [-1,1]\n",
    "])\n",
    "\n",
    "class ZapposImageDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        from PIL import Image\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        # unconditional -> we don't care about labels, return dummy\n",
    "        return img, 0\n",
    "\n",
    "dataset = ZapposImageDataset(image_paths, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Dataset and DataLoader ready.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Time Embedding (Sinusoidal)\n",
    "# -----------------------------\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: (B,) int or float timesteps\n",
    "        returns: (B, dim)\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb_factor = math.log(10000) / (half_dim - 1)\n",
    "        exponents = torch.exp(torch.arange(half_dim, device=device) * -emb_factor)\n",
    "        t = t.float().unsqueeze(1)  # (B,1)\n",
    "        angles = t * exponents[None, :]  # (B, half_dim)\n",
    "        emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1))\n",
    "        return emb\n",
    "\n",
    "# -----------------------------\n",
    "# 3. UNet Building Blocks\n",
    "# -----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        self.time_mlp = nn.Linear(time_dim, out_channels)\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.res_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.res_conv = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        t_emb: (B, time_dim)\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        t_added = self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = h + t_added\n",
    "        h = self.act(self.norm1(h))\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.act(self.norm2(h))\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(in_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "        self.down = nn.Conv2d(out_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        skip = x\n",
    "        x = self.down(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_channels, time_dim):\n",
    "        \"\"\"\n",
    "        in_channels: from below\n",
    "        out_channels: after upsample\n",
    "        skip_channels: from skip connection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "        self.res1 = ResidualBlock(out_channels + skip_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "\n",
    "    def forward(self, x, skip, t_emb):\n",
    "        x = self.up(x)\n",
    "        # fix size mismatch if any (shouldn't happen with 64x64 -> 8x8 pyramid, but safe)\n",
    "        if x.size(-1) != skip.size(-1):\n",
    "            diff = skip.size(-1) - x.size(-1)\n",
    "            x = F.pad(x, (diff // 2, diff - diff // 2, diff // 2, diff - diff // 2))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 4. UNet for 64x64 RGB Shoes (Unconditional)\n",
    "# -----------------------------\n",
    "class ShoeUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=64, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        # Time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_dim),\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim),\n",
    "        )\n",
    "\n",
    "        # Initial conv\n",
    "        self.init_conv = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "\n",
    "        # Encoder: 64x64 -> 32x32 -> 16x16 -> 8x8\n",
    "        chs = [\n",
    "            base_channels,\n",
    "            base_channels * 2,\n",
    "            base_channels * 4,\n",
    "            base_channels * 8,\n",
    "        ]  # [128,256,512,1024]\n",
    "\n",
    "        self.down1 = DownBlock(chs[0], chs[1], time_dim)  # 128->256\n",
    "        self.down2 = DownBlock(chs[1], chs[2], time_dim)  # 256->512\n",
    "        self.down3 = DownBlock(chs[2], chs[3], time_dim)  # 512->1024\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(chs[3], chs[3], time_dim)\n",
    "\n",
    "        # Decoder: 8x8 -> 16x16 -> 32x32 -> 64x64\n",
    "        self.up3 = UpBlock(chs[3], chs[2], skip_channels=chs[3], time_dim=time_dim)\n",
    "        self.up2 = UpBlock(chs[2], chs[1], skip_channels=chs[2], time_dim=time_dim)\n",
    "        self.up1 = UpBlock(chs[1], chs[0], skip_channels=chs[1], time_dim=time_dim)\n",
    "\n",
    "        # Final conv: predict noise ε\n",
    "        self.out_conv = nn.Conv2d(base_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W)\n",
    "        t: (B,) timesteps\n",
    "        \"\"\"\n",
    "        t_emb = self.time_mlp(t)  # (B, time_dim)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        x, skip1 = self.down1(x, t_emb)  # 64x64 -> 32x32\n",
    "        x, skip2 = self.down2(x, t_emb)  # 32x32 -> 16x16\n",
    "        x, skip3 = self.down3(x, t_emb)  # 16x16 -> 8x8\n",
    "\n",
    "        x = self.bottleneck(x, t_emb)\n",
    "\n",
    "        x = self.up3(x, skip3, t_emb)    # 8x8 -> 16x16\n",
    "        x = self.up2(x, skip2, t_emb)    # 16x16 -> 32x32\n",
    "        x = self.up1(x, skip1, t_emb)    # 32x32 -> 64x64\n",
    "\n",
    "        x = self.out_conv(x)             # predict noise\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 5. DDPM Diffusion Utilities\n",
    "# -----------------------------\n",
    "class Diffusion:\n",
    "    def __init__(self, num_steps=1000, beta_start=1e-4, beta_end=0.02, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_steps, device=device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alpha_cumprod_prev = torch.cat(\n",
    "            [torch.tensor([1.0], device=device), self.alpha_cumprod[:-1]], dim=0\n",
    "        )\n",
    "\n",
    "        self.sqrt_alphas = torch.sqrt(self.alphas)\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "\n",
    "        self.posterior_variance = (\n",
    "            self.betas\n",
    "            * (1.0 - self.alpha_cumprod_prev)\n",
    "            / (1.0 - self.alpha_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.clamp(self.posterior_variance, min=1e-20)\n",
    "        )\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_steps, (batch_size,), device=self.device).long()\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: q(x_t | x_0)\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_bar_t = self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_bar_t * x0 + sqrt_one_minus_alpha_bar_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t):\n",
    "        \"\"\"\n",
    "        Reverse step: p(x_{t-1} | x_t)\n",
    "        \"\"\"\n",
    "        B = x_t.size(0)\n",
    "        t_batch = torch.full((B,), t, device=self.device, dtype=torch.long)\n",
    "\n",
    "        eps_theta = model(x_t, t_batch)\n",
    "\n",
    "        beta_t = self.betas[t]\n",
    "        alpha_t = self.alphas[t]\n",
    "        alpha_cumprod_t = self.alpha_cumprod[t]\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alpha_cumprod[t]\n",
    "        sqrt_recip_alpha_t = 1.0 / torch.sqrt(alpha_t)\n",
    "\n",
    "        model_mean = sqrt_recip_alpha_t * (\n",
    "            x_t - (beta_t / sqrt_one_minus_alpha_cumprod_t) * eps_theta\n",
    "        )\n",
    "\n",
    "        if t == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_var_t = self.posterior_variance[t]\n",
    "            noise = torch.randn_like(x_t)\n",
    "            return model_mean + torch.sqrt(posterior_var_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=8):\n",
    "        \"\"\"\n",
    "        Sample x_0 from the model by starting from pure noise.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        x = torch.randn(batch_size, 3, image_size, image_size, device=self.device)\n",
    "\n",
    "        for t in reversed(range(self.num_steps)):\n",
    "            x = self.p_sample(model, x, t)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Instantiate Model & Diffusion\n",
    "# -----------------------------\n",
    "time_dim = 256\n",
    "model = ShoeUNet(\n",
    "    in_channels=3,\n",
    "    base_channels=128,\n",
    "    time_dim=time_dim\n",
    ").to(device)\n",
    "\n",
    "diffusion = Diffusion(\n",
    "    num_steps=1000,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Training Loop\n",
    "# -----------------------------\n",
    "num_epochs = 30  # increase for better visuals\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, _ in train_loader:   # ignore labels\n",
    "        x = x.to(device)        # already normalized to [-1,1]\n",
    "\n",
    "        b = x.size(0)\n",
    "        t = diffusion.sample_timesteps(b)\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        x_t = diffusion.q_sample(x, t, noise=noise)\n",
    "\n",
    "        pred_noise = model(x_t, t)\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * b\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Sampling & Visualization\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def show_samples(model, diffusion, n=8):\n",
    "    \"\"\"\n",
    "    Generate and show shoe samples from the diffusion model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples = diffusion.sample(model, image_size=image_size, batch_size=n)\n",
    "    samples = (samples.clamp(-1, 1) + 1) / 2.0  # back to [0,1]\n",
    "\n",
    "    grid = utils.make_grid(samples, nrow=min(n, 4))\n",
    "    plt.figure(figsize=(4 * (n // 4 + 1), 4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nBase Channels = 128 Results:\")\n",
    "show_samples(model, diffusion, n=8)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
