{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e99a364",
   "metadata": {},
   "source": [
    "# Original working model\n",
    "\n",
    "This is the code for the original model that worked well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a8212",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UT Zappos50K Shoe Diffusion Model (Unconditional DDPM + UNet)\n",
    "# Using images from: ut-zap50k-images-square\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Config & Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "BASE_DIR = \"/root/.cache/kagglehub/datasets/aryashah2k/large-shoe-dataset-ut-zappos50k/versions/1\"\n",
    "IMAGE_ROOT = os.path.join(BASE_DIR, \"ut-zap50k-images-square\")  # we'll use the square images\n",
    "\n",
    "if not os.path.exists(IMAGE_ROOT):\n",
    "    raise FileNotFoundError(f\"IMAGE_ROOT '{IMAGE_ROOT}' does not exist. Check your base path.\")\n",
    "\n",
    "print(\"Image root:\", IMAGE_ROOT)\n",
    "print(\"Contents:\", os.listdir(IMAGE_ROOT)[:10])\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Dataset (custom, recursive file search)\n",
    "# -----------------------------\n",
    "# Collect all image paths recursively from IMAGE_ROOT\n",
    "extensions = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\", \"*.tiff\", \"*.webp\")\n",
    "image_paths = []\n",
    "for ext in extensions:\n",
    "    image_paths.extend(glob.glob(os.path.join(IMAGE_ROOT, \"**\", ext), recursive=True))\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(f\"No image files found under {IMAGE_ROOT} with extensions {extensions}.\")\n",
    "\n",
    "print(f\"Found {len(image_paths)} image files.\")\n",
    "\n",
    "# Transform: resize & normalize (NO cropping, just warp to 64x64)\n",
    "image_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),                     # [0,1]\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),    # [-1,1]\n",
    "])\n",
    "\n",
    "class ZapposImageDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        from PIL import Image\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        # unconditional -> we don't care about labels, return dummy\n",
    "        return img, 0\n",
    "\n",
    "dataset = ZapposImageDataset(image_paths, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Dataset and DataLoader ready.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Time Embedding (Sinusoidal)\n",
    "# -----------------------------\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: (B,) int or float timesteps\n",
    "        returns: (B, dim)\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb_factor = math.log(10000) / (half_dim - 1)\n",
    "        exponents = torch.exp(torch.arange(half_dim, device=device) * -emb_factor)\n",
    "        t = t.float().unsqueeze(1)  # (B,1)\n",
    "        angles = t * exponents[None, :]  # (B, half_dim)\n",
    "        emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1))\n",
    "        return emb\n",
    "\n",
    "# -----------------------------\n",
    "# 3. UNet Building Blocks\n",
    "# -----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        self.time_mlp = nn.Linear(time_dim, out_channels)\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.res_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.res_conv = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        t_emb: (B, time_dim)\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        t_added = self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = h + t_added\n",
    "        h = self.act(self.norm1(h))\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.act(self.norm2(h))\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(in_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "        self.down = nn.Conv2d(out_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        skip = x\n",
    "        x = self.down(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_channels, time_dim):\n",
    "        \"\"\"\n",
    "        in_channels: from below\n",
    "        out_channels: after upsample\n",
    "        skip_channels: from skip connection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "        self.res1 = ResidualBlock(out_channels + skip_channels, out_channels, time_dim)\n",
    "        self.res2 = ResidualBlock(out_channels, out_channels, time_dim)\n",
    "\n",
    "    def forward(self, x, skip, t_emb):\n",
    "        x = self.up(x)\n",
    "        # fix size mismatch if any (shouldn't happen with 64x64 -> 8x8 pyramid, but safe)\n",
    "        if x.size(-1) != skip.size(-1):\n",
    "            diff = skip.size(-1) - x.size(-1)\n",
    "            x = F.pad(x, (diff // 2, diff - diff // 2, diff // 2, diff - diff // 2))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 4. UNet for 64x64 RGB Shoes (Unconditional)\n",
    "# -----------------------------\n",
    "class ShoeUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=64, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        # Time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_dim),\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim),\n",
    "        )\n",
    "\n",
    "        # Initial conv\n",
    "        self.init_conv = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "\n",
    "        # Encoder: 64x64 -> 32x32 -> 16x16 -> 8x8\n",
    "        chs = [\n",
    "            base_channels,\n",
    "            base_channels * 2,\n",
    "            base_channels * 4,\n",
    "            base_channels * 8,\n",
    "        ]  # [64,128,256,512]\n",
    "\n",
    "        self.down1 = DownBlock(chs[0], chs[1], time_dim)  # 64->128\n",
    "        self.down2 = DownBlock(chs[1], chs[2], time_dim)  # 128->256\n",
    "        self.down3 = DownBlock(chs[2], chs[3], time_dim)  # 256->512\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(chs[3], chs[3], time_dim)\n",
    "\n",
    "        # Decoder: 8x8 -> 16x16 -> 32x32 -> 64x64\n",
    "        self.up3 = UpBlock(chs[3], chs[2], skip_channels=chs[3], time_dim=time_dim)\n",
    "        self.up2 = UpBlock(chs[2], chs[1], skip_channels=chs[2], time_dim=time_dim)\n",
    "        self.up1 = UpBlock(chs[1], chs[0], skip_channels=chs[1], time_dim=time_dim)\n",
    "\n",
    "        # Final conv: predict noise Îµ\n",
    "        self.out_conv = nn.Conv2d(base_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W)\n",
    "        t: (B,) timesteps\n",
    "        \"\"\"\n",
    "        t_emb = self.time_mlp(t)  # (B, time_dim)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        x, skip1 = self.down1(x, t_emb)  # 64x64 -> 32x32\n",
    "        x, skip2 = self.down2(x, t_emb)  # 32x32 -> 16x16\n",
    "        x, skip3 = self.down3(x, t_emb)  # 16x16 -> 8x8\n",
    "\n",
    "        x = self.bottleneck(x, t_emb)\n",
    "\n",
    "        x = self.up3(x, skip3, t_emb)    # 8x8 -> 16x16\n",
    "        x = self.up2(x, skip2, t_emb)    # 16x16 -> 32x32\n",
    "        x = self.up1(x, skip1, t_emb)    # 32x32 -> 64x64\n",
    "\n",
    "        x = self.out_conv(x)             # predict noise\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 5. DDPM Diffusion Utilities\n",
    "# -----------------------------\n",
    "class Diffusion:\n",
    "    def __init__(self, num_steps=1000, beta_start=1e-4, beta_end=0.02, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_steps, device=device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alpha_cumprod_prev = torch.cat(\n",
    "            [torch.tensor([1.0], device=device), self.alpha_cumprod[:-1]], dim=0\n",
    "        )\n",
    "\n",
    "        self.sqrt_alphas = torch.sqrt(self.alphas)\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "\n",
    "        self.posterior_variance = (\n",
    "            self.betas\n",
    "            * (1.0 - self.alpha_cumprod_prev)\n",
    "            / (1.0 - self.alpha_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.clamp(self.posterior_variance, min=1e-20)\n",
    "        )\n",
    "\n",
    "    def sample_timesteps(self, batch_size):\n",
    "        return torch.randint(0, self.num_steps, (batch_size,), device=self.device).long()\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: q(x_t | x_0)\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_bar_t = self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_bar_t * x0 + sqrt_one_minus_alpha_bar_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t):\n",
    "        \"\"\"\n",
    "        Reverse step: p(x_{t-1} | x_t)\n",
    "        \"\"\"\n",
    "        B = x_t.size(0)\n",
    "        t_batch = torch.full((B,), t, device=self.device, dtype=torch.long)\n",
    "\n",
    "        eps_theta = model(x_t, t_batch)\n",
    "\n",
    "        beta_t = self.betas[t]\n",
    "        alpha_t = self.alphas[t]\n",
    "        alpha_cumprod_t = self.alpha_cumprod[t]\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alpha_cumprod[t]\n",
    "        sqrt_recip_alpha_t = 1.0 / torch.sqrt(alpha_t)\n",
    "\n",
    "        model_mean = sqrt_recip_alpha_t * (\n",
    "            x_t - (beta_t / sqrt_one_minus_alpha_cumprod_t) * eps_theta\n",
    "        )\n",
    "\n",
    "        if t == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_var_t = self.posterior_variance[t]\n",
    "            noise = torch.randn_like(x_t)\n",
    "            return model_mean + torch.sqrt(posterior_var_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=8):\n",
    "        \"\"\"\n",
    "        Sample x_0 from the model by starting from pure noise.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        x = torch.randn(batch_size, 3, image_size, image_size, device=self.device)\n",
    "\n",
    "        for t in reversed(range(self.num_steps)):\n",
    "            x = self.p_sample(model, x, t)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Instantiate Model & Diffusion\n",
    "# -----------------------------\n",
    "time_dim = 256\n",
    "model = ShoeUNet(\n",
    "    in_channels=3,\n",
    "    base_channels=64,\n",
    "    time_dim=time_dim\n",
    ").to(device)\n",
    "\n",
    "diffusion = Diffusion(\n",
    "    num_steps=1000,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Training Loop\n",
    "# -----------------------------\n",
    "num_epochs = 30  # increase for better visuals\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, _ in train_loader:   # ignore labels\n",
    "        x = x.to(device)        # already normalized to [-1,1]\n",
    "\n",
    "        b = x.size(0)\n",
    "        t = diffusion.sample_timesteps(b)\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        x_t = diffusion.q_sample(x, t, noise=noise)\n",
    "\n",
    "        pred_noise = model(x_t, t)\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * b\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Sampling & Visualization\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def show_samples(model, diffusion, n=8):\n",
    "    \"\"\"\n",
    "    Generate and show shoe samples from the diffusion model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples = diffusion.sample(model, image_size=image_size, batch_size=n)\n",
    "    samples = (samples.clamp(-1, 1) + 1) / 2.0  # back to [0,1]\n",
    "\n",
    "    grid = utils.make_grid(samples, nrow=min(n, 4))\n",
    "    plt.figure(figsize=(4 * (n // 4 + 1), 4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nAfter training, call `show_samples(model, diffusion, n=8)` to visualize generated shoes.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
